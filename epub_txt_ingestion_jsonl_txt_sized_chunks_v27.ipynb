{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMb4jKySFelk2Gmlwdb/EkD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["(work progress draft, but it works)\n","- add chunk overlapping!\n","- add period edge cases like Dr. is not the end of a sentence."],"metadata":{"id":"NgeR47O8mn-l"}},{"cell_type":"markdown","source":[],"metadata":{"id":"MQ4a1jR46XZ0"}},{"cell_type":"markdown","source":["# Training & Retrieval Material: Personal Data Set Creation\n","A significant problem and gap between public-mega-model AI and private real-world-use is that a combination of private and copyrighted material comprise a large part of what private individuals and groups need to use AI for, and by definition this is what public-mega-ai is prohibited from being able to help with (sometimes for good reasons).\n","\n","One way of looking at this is the Clear-Web vs. Deep-Web gap which becomes semantially elusive. When people say 'the internet' or 'the whole web' it sounds like they are talking about most of the material in the world, or at least 'the whole internet' but as they say: Naming things is hard. The entire-public-internet is not the same as the-entire-internet. Training on 'the entire public internet' actually means about 4% of the internet which is the public \"clear-web.\" The rest of the internet is the other 95% of the internet (or Deep-web) and is not public, let alone all the material not on the internet at all (which for many institutions is most of their files). (And then about 0.5% is the dark web...but we're not sure how big the dark web is because it's so hard to see any of it at all.)\n","\n","To help square this circle, the proposed method here is to make available a pipeline by which people can turn their private or personally owned documents into formatted Training & Retreival Material which can be used to train, fine-tune, and make databases for retrieval augumented uses of AI (which may be generative or more than just generative). (No doubt there are other very similar projects.)"],"metadata":{"id":"6H5CfuxfXVIC"}},{"cell_type":"markdown","source":["# Smart chunking\n","\n","python code\n","\n","a system in python to ingest an epub book into a .txt file\n","\n","code in python to 'chunk' this text with the following policies and rules.\n","\n","1. redundancy/overlap between chunks (no system yet, may vary widely by context of use, e.g. RAG or fine tune)\n","2. smart-chunking where the chunk will not be cut in random places (cutting words or phrases in half)\n","3. try to identify paragraph ends to make a cut (not done yet)\n","4. try to identify sentence ends to make a cut (currrent norm)\n","5. specifically check that the word 'not' is not isolated at or within ~25 characters to either side of a cut (so that negative meanings are not invented or lost to redacted context) (should be covered by sentence-based cuts.)\n","6. specified chunk max size (defaults to 500 char, set to any input)\n","7. specified overlap size\n","8. method of padding for exact chunk size (not done yet)\n","9. look at how code and other non-sentence material is cut (not done yet)\n","\n","\n","iterate through chapter files...\n","maybe break up text if the text is too large...issue.\n","\n","\n","\n","When files are too long to chunk(by the first method), and so need to be chunked (by another method):\n","\n","So perhaps if a file/string's length is above say, 10,000 characters.\n","\n","Steps:\n","1. Look at a range of ~500 characters the middle of the file loaded as a string: e.g.\n","```\n","middle_section_start = string[len)//2 - 250\n","middle_section_end = string[len)//2 + 250\n","string[middle_section_start:middle_section_end]\n","```\n","\n","Within that section, look for the end of a paragraph or end of a sentence.\n","\n","\". \" or \".\\n\"\n","\n","find the first likely end of a sentence of paragraph in that\n","middle of a block of text.\n","\n","\n","try to avoid sub-sences that are intside quotes or parenthesis\n","\"yyy.\"\n","(yyy?)\n","\n","What to do when a sentence itself is more than the chunk size...\n","\n","for freak-sentences that are longer then 500 char...\n","maybe redundantly splits the sentences avoiding mid-word and 'not' cuts\n","\n","\n"],"metadata":{"id":"WB6jOKNFGEQc"}},{"cell_type":"markdown","source":["# Metadata:\n","\n","## Maybe Goal:\n","Generate a directory of metadata per chunk,\n","so that from this menue of meta-data specific ~versions of the chunk can be created,\n","e.g.\n","- for graph-db\n","- for vector-db\n","- for formats of training-data:\n","-- instruct\n","-- Q&A\n","-- dialogue\n","-- raw input\n","-- augmented input(s)\n","\n","Project:\n","epub should be an excellent format for providing specific metadata for chunks of the text, as there is an index, a table of contents, and often information about where the chunk exists in the content structure.\n","\n","Structural Mapping:  Analyze the EPUB's structure (table of contents, chapters, sections, index) to locate relevant text chunks and establish their hierarchy.\n","\n","metadata may be used (or designed specifically for):\n","- vector DB RAG\n","- graph DB RAG\n","- Q&A format model training / fine tuning dataset\n","- instruction based model training / fine tuning dataset\n","- maybe text with meta-data context for simple text model training / fine tuning dataset\n","\n","possible metatdata areas\n","summary:\n","example question:\n","name of text:\n","name of chapter/section:\n","topic of chapter/section:\n","topic links from book index:\n","\n","Tasks:\n","- make a Metadata Schema\n","- have parameters for what kinds of metadata are created\n","\n","- focus on the initial task of what meta-data can be derived directly from the epub files such as table of contents(at the start) and index (at the end) both of which have hyperlinks to connect text and terms.\n","\n","\n","Note: chunk size may need to shrinkk if chunk-metadata is"],"metadata":{"id":"DNsBC_YUsMDn"}},{"cell_type":"markdown","source":["Why Structural Metadata Matters\n","\n","Enhanced Navigation: TOC and index serve as crucial tools for readers to quickly navigate within a complex document and locate specific information.\n","Improved Discoverability: Index entries and keywords enhance the visibility of your EPUB within search engines and digital libraries.\n","Semantic Structure: Structural metadata adds a layer of semantic information to the text, improving machine understanding of the document content. This is valuable for the kinds of AI applications you mentioned earlier."],"metadata":{"id":"KDJtv7iZ_2rG"}},{"cell_type":"markdown","source":["### Metadata Schema Design\n","\n","\n","## Specific Sources:\n","- Table of Contents (TOC): A hierarchical structure representing the document's layout, including chapters, sections, and subsections.\n","- (sometimes) detailed table of contents\n","- Index: A list of terms, topics, and names with hyperlinks to their occurrences in the document.\n","- Chapter/Section introductions\n","\n","\n","## 1. Basic Metadata Items:\n","- Title: The name of the EPUB document.\n","- Author(s): The creator(s) of the document.\n","- Publisher: The publisher of the document.\n","- Edition\n","- Publication Date: When the document was published.\n","- Language: The primary language of the document.\n","- ISBN: The International Standard Book Number, if available.\n","\n","## 2. Structural Metadata Items:\n","- section_name: The title of the chapter or section.\n","- ID/Reference: A unique identifier or reference within the document.\n","- Topic(s): Main topics covered in the chapter or section.\n","- Keywords: A list of key terms associated with the chapter or section.\n","\n","## 3. Content-Specific Metadata\n","- Summary: A brief overview of the document or specific sections/chapters.\n","- Example Questions: Potential questions that the content can answer, useful for Q&A model training.\n","- Topic Links: Hyperlinks within the index that connect to specific topics, terms, or sections in the document.\n","\n","## 4. Advanced Metadata for Machine Learning\n","- Vector Representations: Metadata that includes vector representations of chapters or sections, useful for vector databases and similarity searches.\n","- Graph Connections: Information on how different sections are related or can be linked, valuable for graph databases.\n","- Instruction Metadata: Specific instructions or tasks that can be derived from the content, useful for instruction-based model training."],"metadata":{"id":"fc3NHJda-yfb"}},{"cell_type":"markdown","source":["# challenges\n","\n","1. RAG DB: metadata for graph-DB and for vector-DB are very different in their use and requirements\n","\n","2. Size: if metadata, or some metadata, is included in the chunk, that will shrink the chunk itself. E.g. max 500 char = 200 char of metadata + 300 char of actual document text\n","\n","3. pragmatism: the low hanging fruit of epub data extraction is not clearly\n","useful:\n","```\n","def extract_global_metadata(opf_content):\n","    tree = ET.ElementTree(ET.fromstring(opf_content))\n","    root = tree.getroot()\n","    ns = {'dc': 'http://purl.org/dc/elements/1.1/'}\n","    \n","    global_metadata = {\n","        'title': root.find('.//dc:title', ns).text,\n","        'author': root.find('.//dc:creator', ns).text,\n","        'publisher': root.find('.//dc:publisher', ns).text,\n","        'publication_date': root.find('.//dc:date', ns).text,\n","        'language': root.find('.//dc:language', ns).text\n","    }\n","    \n","    return global_metadata\n","```\n","```\n","def append_chunks_to_jsonl(chunks_list, output_chunks_jsonl_path, chunk_source_name, metadata):\n","    with open(output_chunks_jsonl_path, 'a') as f:\n","        for index, this_chunk in enumerate(chunks_list):\n","            chunk_data = {\n","                \"source_name\": f\"{chunk_source_name}_{index}\",\n","                \"text\": this_chunk,\n","                \"metadata\": metadata  # Include the metadata here\n","            }\n","            f.write(json.dumps(chunk_data) + '\\n')\n","\n","```"],"metadata":{"id":"ljoKt5hYMJ6m"}},{"cell_type":"code","source":[],"metadata":{"id":"C-pmA8Wr-oxT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TODO chat-ML formatting for dataset processing of chunked jsonl with metadata\n","\n","e.g.\n","\n","terms from index\n","\n","chunk:\n","author:\n","book name:\n","section/chapter:\n","index key words:\n","\n","user: please tell me more about {if fields: \"including {index key words}\"}} in {author's}{book title} section {chapter/section}\n","\n","assistant: In {chunk}\n","\n","...\n","\n","This comes from the example of using color descriptions to make a 'color expert' chatbot\n","\n","# Task:\n","- looking up index terms\n","- possibly connecting sections to other sections (real-graph? or vector-gossip-graph?)\n"],"metadata":{"id":"2CILGyvH-pQ5"}},{"cell_type":"markdown","source":["To Jan:\n","\n","Feature Request: allow epub books\n","\n","Hopefully code like this (see most recent version)\n","https://github.com/lineality/epub_ingestion_python/blob/main/epub_injestion_jsonl_txt_sized_chunks_v21.py\n","\n","Will be useful in letting users of Jan use epub-books with their Jan Retrieval function.\n","\n","This code extracts text from epub books into a variety of formats, txt json, jsonl, and can chunk to specific sizes without cutting words or sentences in half, to better retain meaning."],"metadata":{"id":"bfpjye87rpYn"}},{"cell_type":"code","source":["#########################################################\n","# This code automaticaly finds and processes epub books\n","# esspecially for RAG document ingestion processing\n","#########################################################\n","\"\"\"\n","input -> one or more epub files\n","output -> txt and json files that contain the text from sections of the book\n","          as well as smart-chunked segments made to your size specs\n","          e.g. a max of 500 characters, which contain whole sentences.\n","          Chunks do not cut words or sentences in half.\n","\n","# Set of Results, saved in a file per epub doc:\n","1. One .jsonl file\n","2. (Plural) Individual .json files in a folder\n","3. One .txt text file of the whole epub\n","4. (plural) Individual .txt files from separate parts of epub\n","5. chunks as one .jsonl file\n","6. (Plural) chunks under specified character length as separate text\n","Future Feature:\n","7. Chunk-Metadata (for model training, for DB-retrieval, etc.)\n","\"\"\"\n","\n","import zipfile\n","import xml.etree.ElementTree as ET\n","from bs4 import BeautifulSoup\n","import json\n","import os\n","import shutil\n","import re\n","\n","def get_ordered_html_files(opf_content):\n","    \"\"\"\n","    Parses the content.opf file to determine the reading order of HTML files in the EPUB.\n","\n","    The function reads the 'content.opf' file, which contains metadata about the EPUB's structure.\n","    It identifies the 'spine' element, which lists the reading order of the content documents,\n","    and the 'manifest' element, which provides the location of these documents.\n","    The function returns a list of HTML file paths in the order they should be read.\n","\n","    Args:\n","    opf_content (str): A string representation of the content.opf file.\n","\n","    Returns:\n","    list: An ordered list of HTML file paths as specified in the EPUB's spine.\n","    \"\"\"\n","\n","    # Parse the content.opf XML content\n","    tree = ET.ElementTree(ET.fromstring(opf_content))\n","    root = tree.getroot()\n","\n","    # Define the namespace for the OPF package file\n","    ns = {'opf': 'http://www.idpf.org/2007/opf'}\n","\n","    # Find the spine element which indicates the order of the content documents\n","    spine = root.find('opf:spine', ns)\n","    itemrefs = spine.findall('opf:itemref', ns)\n","\n","    # Extract the id references for each item in the spine\n","    item_ids = [itemref.get('idref') for itemref in itemrefs]\n","\n","    # Find the manifest element which lists all the content documents\n","    manifest = root.find('opf:manifest', ns)\n","    items = manifest.findall('opf:item', ns)\n","\n","    # Create a dictionary mapping item IDs to their corresponding file paths\n","    html_files = {item.get('id'): item.get('href') for item in items if item.get('media-type') == 'application/xhtml+xml'}\n","\n","    # Generate an ordered list of HTML files based on the spine order\n","    ordered_html_files = [html_files[item_id] for item_id in item_ids if item_id in html_files]\n","\n","    return ordered_html_files\n","\n","\n","def extract_text_from_html(html_content, this_epub_output_dir_path):\n","    \"\"\"\n","    Extracts and returns text from an HTML content.\n","    \"\"\"\n","    # print(\"HTML Content before BeautifulSoup Parsing:\\n\", html_content[:500])  # Print first 500 characters of HTML\n","    print_and_log(f\"\\nlen(HTML Content before BeautifulSoup Parsing) -> {len(html_content)}\", this_epub_output_dir_path)  # Print first 500 characters of HTML\n","\n","\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    parsed_text = soup.get_text()\n","    # print(\"Extracted Text:\\n\", parsed_text[:500])  # Print first 500 characters of extracted text\n","    print_and_log(f\"len(Extracted Text) -> {len(parsed_text)}\", this_epub_output_dir_path)  # Print first 500 characters of extracted text\n","\n","    return parsed_text\n","\n","def fix_text_formatting(text):\n","    \"\"\"Replaces the Unicode right single quotation mark with a standard apostrophe.\"\"\"\n","    return text.replace(\"\\u2019\", \"'\")\n","\n","\n","def check_len_chunks_in_list(chunks_list, max_chunk_size, this_epub_output_dir_path):\n","\n","    size_flag_ok = True\n","\n","    for index, this_chunk in enumerate(chunks_list):\n","\n","        # get size of chunk\n","        this_length = len( this_chunk )\n","\n","        # check size against user-input max size\n","        if this_length > max_chunk_size:\n","            print_and_log( this_length, this_epub_output_dir_path )\n","            print_and_log( f\"\"\"\n","            Warning: chunk over max size.\n","            This chunk size: {this_chunk}.\n","            Max size: {max_chunk_size}\n","            \"\"\" )\n","            print_and_log( f\"This chunk: {this_chunk}\", this_epub_output_dir_path )\n","            size_flag_ok = False\n","\n","    # report and log\n","    if size_flag_ok:\n","        print_and_log( \"Size Check, OK \\\\o/\", this_epub_output_dir_path )\n","    else:\n","        print_and_log( \"WARNING: Size Check Failed!\", this_epub_output_dir_path )\n","\n","\n","\n","def save_individual_chunks(chunks_list, output_chunks_dir, chunk_source_name):\n","\n","    for index, this_chunk in enumerate(chunks_list):\n","        chunk_name = f\"{chunk_source_name}_{index}.txt\"\n","\n","        # Save individual txt files\n","        individual_chunk_path = os.path.join(output_chunks_dir, chunk_name)\n","        with open(individual_chunk_path, 'w') as f:\n","            f.write(this_chunk)\n","            # print('chunk writen', individual_chunk_path)\n","\n","    return len(chunks_list)\n","\n","\n","def append_chunks_to_jsonl(chunks_list, output_chunks_jsonl_path, chunk_source_name):\n","    \"\"\"Appends chunks of text to a .jsonl file, each chunk as a JSON object.\n","\n","    Args:\n","        chunks_list (list): List of text chunks to be appended.\n","        output_jsonl_path (str): The output file path for the .jsonl file.\n","        chunk_source_name (str): Base name for each chunk, used in the 'source_name' field.\n","\n","    Returns:\n","        int: The number of chunks appended.\n","    \"\"\"\n","\n","    with open(output_chunks_jsonl_path, 'a') as f:  # Open file in append mode\n","        for index, this_chunk in enumerate(chunks_list):\n","            # Construct a JSON object for the chunk\n","            chunk_data = {\n","                \"source_name\": f\"{chunk_source_name}_{index}\",\n","                \"text\": this_chunk\n","            }\n","\n","            # Convert the chunk data to a JSON string and append it to the file with a newline\n","            f.write(json.dumps(chunk_data) + '\\n')\n","\n","    return len(chunks_list)\n","\n","\n","def print_and_log(input_text, this_epub_output_dir_path):\n","    # check if input is a string, if not...make it a string!\n","    if not isinstance(input_text, str):\n","        input_text = str(input_text)\n","\n","    # print to terminal\n","    print(input_text)\n","\n","    # log file path is...\n","    log_file_path = os.path.join(this_epub_output_dir_path, \"log.txt\")\n","\n","    # log: Write/Append to a log.txt file\n","    with open(log_file_path, 'a') as f:\n","        f.write(input_text + '\\n\\n')\n","\n","\n","def extract_text_from_epub(epub_file_path, this_epub_output_dir_path, output_jsonl_path, output_json_dir, output_whole_txt_path, output_txt_dir, output_chunks_jsonl_path, output_chunks_dir, max_chunk_size=500):\n","    with zipfile.ZipFile(epub_file_path, 'r') as epub:\n","        print_and_log(f\"EPUB Contents: -> {epub.namelist()}\", this_epub_output_dir_path)\n","\n","        ###################\n","        # Make Directories\n","        ###################\n","\n","        # Create a directory for individual JSON files\n","        if not os.path.exists(output_json_dir):\n","            os.makedirs(output_json_dir)\n","\n","        # Create a directory for individual txt files\n","        if not os.path.exists(output_txt_dir):\n","            os.makedirs(output_txt_dir)\n","\n","        # Create a directory for chunks output_chunks_dir\n","        if not os.path.exists(output_chunks_dir):\n","            os.makedirs(output_chunks_dir)\n","\n","\n","        ##################################\n","        # Get & Read html files from epub\n","        ##################################\n","        # find opf file\n","        opf_file = [f for f in epub.namelist() if 'content.opf' in f][0]\n","\n","        # read opf file\n","        opf_content = epub.read(opf_file).decode('utf-8')\n","\n","        # get ordered HTML files\n","        ordered_html_files_list = get_ordered_html_files(opf_content)\n","\n","\n","        ############################################\n","        # Read and extract text from each HTML file\n","        ############################################\n","\n","        # iterate through html files\n","        for html_file in ordered_html_files_list:\n","            full_path = os.path.join(os.path.dirname(opf_file), html_file)\n","            if full_path in epub.namelist():\n","                html_content = epub.read(full_path).decode('utf-8')\n","\n","                #########################\n","                # extract text from epub\n","                #########################\n","                raw_text = extract_text_from_html(html_content, this_epub_output_dir_path)\n","                print_and_log(f\"len(text for json)-> {len(raw_text)}\", this_epub_output_dir_path)\n","\n","                # fix text formatting\n","                text = fix_text_formatting(raw_text)\n","\n","\n","                #################\n","                # .json & .jsonl\n","                #################\n","\n","                # Write/Append to a single JSONL file\n","                with open(output_jsonl_path, 'a') as f:\n","                    json_record = json.dumps({'text': text.strip()})\n","                    f.write(json_record + '\\n')\n","\n","                # Save individual JSON file\n","                individual_json_path = os.path.join(output_json_dir, f\"{os.path.splitext(html_file)[0]}.json\")\n","                with open(individual_json_path, 'w') as f:\n","                    json.dump({'text': text.strip()}, f, indent=4)\n","\n","                #######\n","                # .txt\n","                #######\n","\n","                # Write/Append to a single text .txt file\n","                with open(output_whole_txt_path, 'a') as f:\n","                    f.write(text + '\\n\\n')\n","\n","                # Save individual txt files\n","                individual_txt_path = os.path.join(output_txt_dir, f\"{os.path.splitext(html_file)[0]}.txt\")\n","                with open(individual_txt_path, 'w') as f:\n","                    f.write(text)\n","\n","                #########\n","                # chunks\n","                #########\n","\n","                chunks_list = make_chunk_list(text, max_chunk_size, this_epub_output_dir_path)\n","\n","                chunk_source_name = os.path.splitext(html_file)[0]\n","\n","                # check sizes\n","                check_len_chunks_in_list(chunks_list, max_chunk_size, this_epub_output_dir_path)\n","\n","                number_of_chunks = save_individual_chunks(chunks_list, output_chunks_dir, chunk_source_name)\n","                print_and_log(f\"Chunked: split into this many chunks-> {number_of_chunks}\", this_epub_output_dir_path)\n","\n","                append_chunks_to_jsonl(chunks_list, output_chunks_jsonl_path, chunk_source_name)\n","\n","                print_and_log(f\"{html_file} -> ok!\", this_epub_output_dir_path)\n","\n","\n","            else:  # File Not Found\n","                print_and_log(f\"Warning: File {full_path} not found in the archive.\")\n","\n","\n","\n","def extract_text_from_txt(txt_file_path, this_txt_output_dir_path, output_jsonl_path, output_json_dir, output_whole_txt_path, output_txt_dir, output_chunks_jsonl_path, output_chunks_dir, max_chunk_size=500):\n","\n","    # Open the file in read mode\n","    with open(txt_file_path, 'r') as file:\n","\n","        ###################\n","        # Make Directories\n","        ###################\n","\n","        # Create a directory for individual JSON files\n","        if not os.path.exists(output_json_dir):\n","            os.makedirs(output_json_dir)\n","\n","        # Create a directory for individual txt files\n","        if not os.path.exists(output_txt_dir):\n","            os.makedirs(output_txt_dir)\n","\n","        # Create a directory for chunks output_chunks_dir\n","        if not os.path.exists(output_chunks_dir):\n","            os.makedirs(output_chunks_dir)\n","\n","\n","\n","        #########################\n","        # extract text from epub\n","        #########################\n","        # Read the entire contents of the file\n","        text = file.read()\n","\n","        #################\n","        # .json & .jsonl\n","        #################\n","\n","        # Write/Append to a single JSONL file\n","        with open(output_jsonl_path, 'a') as f:\n","            json_record = json.dumps({'text': text.strip()})\n","            f.write(json_record + '\\n')\n","\n","        # Save individual JSON file\n","        individual_json_path = os.path.join(output_json_dir, f\"{os.path.splitext(txt_file_path)[0]}.json\")\n","        with open(individual_json_path, 'w') as f:\n","            json.dump({'text': text.strip()}, f, indent=4)\n","\n","        #######\n","        # .txt\n","        #######\n","\n","        # Write/Append to a single text .txt file\n","        with open(output_whole_txt_path, 'a') as f:\n","            f.write(text + '\\n\\n')\n","\n","        # Save individual txt files\n","        individual_txt_path = os.path.join(output_txt_dir, f\"{os.path.splitext(txt_file_path)[0]}.txt\")\n","        with open(individual_txt_path, 'w') as f:\n","            f.write(text)\n","\n","\n","        #########\n","        # chunks\n","        #########\n","\n","        chunks_list = make_chunk_list(text, max_chunk_size, this_txt_output_dir_path)\n","\n","        chunk_source_name = os.path.splitext(txt_file_path)[0]\n","\n","        # check sizes\n","        check_len_chunks_in_list(chunks_list, max_chunk_size, this_txt_output_dir_path)\n","\n","        number_of_chunks = save_individual_chunks(chunks_list, output_chunks_dir, chunk_source_name)\n","        print_and_log(f\"Chunked: split into this many chunks-> {number_of_chunks}\", this_txt_output_dir_path)\n","\n","        append_chunks_to_jsonl(chunks_list, output_chunks_jsonl_path, chunk_source_name)\n","\n","        print_and_log(f\"{txt_file_path} -> ok!\", this_txt_output_dir_path)\n","\n","        print(\"OK!\")\n","\n","\n","\n","\n","def zip_folder(path_to_directory_to_zip, output_destination_zip_file_path):\n","    \"\"\"Creates a zip archive of a specified folder.\n","\n","    Args:\n","        path_to_directory_to_zip (str): The path to the folder to be zipped.\n","        output_destination_zip_file_path (str): The desired name and path of the output zip file.\n","    \"\"\"\n","    # # Specify the folder you want to zip\n","    # path_to_directory_to_zip = \"individual_jsons\"\n","\n","    # # Specify the desired output zip file name (e.g., 'jsons_archive.zip')\n","    # output_destination_zip_file_path = \"jsons_archive_zip\"\n","\n","    shutil.make_archive(output_destination_zip_file_path, 'zip', path_to_directory_to_zip)\n","\n","\n","###########\n","# Chunking\n","############\n","\n","\n","def split_sentences_and_punctuation(text):\n","    \"\"\"Splits text into sentences, attempting to preserve punctuation and all text content.\n","    Args:\n","        text (str): The input text.\n","    Returns:\n","        list: A list of sentences with preserved punctuation.\n","    \"\"\"\n","\n","    ABBREVIATIONS = [\n","    \"Dr.\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Lt.\", \"St.\", \"Capt.\", \"Col.\", \"Gen.\", \"Rev.\", \"Hon.\",\n","    \"Jr.\", \"Sr.\", \"Ph.D.\", \"M.D.\", \"B.A.\", \"M.A.\", \"D.D.S.\", \"J.D.\", \"M.B.A.\", \"C.P.A.\",\n","    \"a.m.\", \"p.m.\", \"etc.\", \"i.e.\", \"e.g.\", \"vs.\", \"et al.\", \"approx.\", \"circa\", \"fig.\",\n","    \"Jan.\", \"Feb.\", \"Mar.\", \"Apr.\", \"May.\", \"Jun.\", \"Jul.\", \"Aug.\", \"Sep.\", \"Oct.\", \"Nov.\", \"Dec.\",\n","    \"Prof.\", \"Assoc.\", \"Asst.\", \"Dept.\", \"Univ.\", \"Gov.\", \"Sen.\", \"Rep.\", \"Pres.\",\n","    \"Vol.\", \"No.\", \"Pt.\", \"Ch.\", \"Sec.\", \"Subsec.\", \"Para.\", \"Subpara.\", \"Cl.\", \"Subcl.\",\n","    \"Ex.\", \"Exs.\", \"Fig.\", \"Figs.\", \"Eq.\", \"Eqs.\", \"Thm.\", \"Thms.\", \"Prop.\", \"Props.\",\n","    \"Def.\", \"Defs.\", \"Cor.\", \"Cors.\", \"Lem.\", \"Lems.\", \"Proof.\", \"Proofs.\",\n","    \"min.\", \"hrs.\", \"sec.\", \"yr.\", \"yrs.\", \"mo.\", \"mos.\", \"wk.\", \"wks.\",\n","    \"ft.\", \"in.\", \"cm.\", \"mm.\", \"kg.\", \"g.\", \"oz.\", \"lb.\", \"lbs.\", \"ml.\", \"l.\",\n","    \"A.\", \"B.\", \"C.\", \"D.\", \"E.\", \"F.\", \"G.\", \"H.\", \"I.\", \"J.\", \"K.\", \"L.\", \"M.\",\n","    \"N.\", \"O.\", \"P.\", \"Q.\", \"R.\", \"S.\", \"T.\", \"U.\", \"V.\", \"W.\", \"X.\", \"Y.\", \"Z.\",\n","    \"1.\", \"2.\", \"4.\", \"4.\", \"5.\", \"6.\", \"7.\", \"8.\", \"9.\",\n","    \"10.\", \"20.\", \"30.\", \"40.\", \"50.\", \"60.\", \"70.\", \"80.\", \"90.\",\n","    \"11.\", \"12.\", \"13.\", \"14.\", \"15.\", \"16.\", \"17.\", \"18.\", \"19.\",\n","    \"21.\", \"22.\", \"23.\", \"24.\", \"25.\", \"26.\", \"27.\", \"28.\", \"29.\",\n","    \"31.\", \"32.\", \"33.\", \"34.\", \"35.\", \"36.\", \"37.\", \"38.\", \"39.\",\n","    \"41.\", \"42.\", \"43.\", \"44.\", \"45.\", \"46.\", \"47.\", \"48.\", \"49.\",\n","    \"51.\", \"52.\", \"53.\", \"54.\", \"55.\", \"56.\", \"57.\", \"58.\", \"59.\",\n","    \"61.\", \"62.\", \"63.\", \"64.\", \"65.\", \"66.\", \"67.\", \"68.\", \"69.\",\n","    \"71.\", \"72.\", \"73.\", \"74.\", \"75.\", \"76.\", \"77.\", \"78.\", \"79.\",\n","    \"81.\", \"82.\", \"83.\", \"84.\", \"85.\", \"86.\", \"87.\", \"88.\", \"89.\",\n","    \"91.\", \"92.\", \"93.\", \"94.\", \"95.\", \"96.\", \"97.\", \"98.\", \"99.\",\n","\n","    \"I.\", \"II.\", \"III.\", \"IV.\", \"V.\", \"VI.\", \"VII.\", \"VIII.\", \"IX.\", \"X.\",\n","    \"XI.\", \"XII.\", \"XIII.\", \"XIV.\", \"XV.\", \"XVI.\", \"XVII.\", \"XVIII.\", \"XIX.\", \"XX.\",\n","    \"i.\", \"ii.\", \"iii.\", \"iv.\", \"v.\", \"vi.\", \"vii.\", \"viii.\", \"ix.\", \"x.\",\n","    \"xi.\", \"xii.\", \"xiii.\", \"xiv.\", \"xv.\", \"xvi.\", \"xvii.\", \"xviii.\", \"xix.\", \"xx.\",\n","    \"a.\", \"b.\", \"c.\", \"d.\", \"e.\", \"f.\", \"g.\", \"h.\", \"i.\", \"j.\", \"k.\", \"l.\", \"m.\",\n","    \"n.\", \"o.\", \"p.\", \"q.\", \"r.\", \"s.\", \"t.\", \"u.\", \"v.\", \"w.\", \"x.\", \"y.\", \"z.\"\n","    ]\n","\n","\n","    # Construct a pattern to match abbreviations\n","    abbreviations_pattern = r\"(?<=\\b(?:{})\\b)\".format(\"|\".join(re.escape(abbr) for abbr in ABBREVIATIONS))\n","\n","    # This pattern attempts to split at sentence endings (.?!), including the punctuation with the preceding sentence\n","    # It uses a lookahead to keep the punctuation with the sentence\n","    # The negative lookbehind (?<!{abbreviations_pattern}) excludes known abbreviations from being split\n","    sentence_end_regex = r'(?<!{abbreviations_pattern})(?<=[.!?])\\s+(?=[A-Z])'.format(abbreviations_pattern=abbreviations_pattern)\n","\n","    split_sentences_and_punctuation_list = re.split(sentence_end_regex, text)\n","\n","    # Optionally, remove empty strings if they are not desired\n","    split_sentences_and_punctuation_list = [s for s in split_sentences_and_punctuation_list if s]\n","\n","    return split_sentences_and_punctuation_list\n","\n","def split_sentences_and_punctuation(text):\n","    \"\"\"Splits text into sentences, attempting to preserve punctuation and all text content.\n","    Args:\n","        text (str): The input text.\n","    Returns:\n","        list: A list of sentences with preserved punctuation.\n","\n","\n","    ABBREVIATIONS\n","    \"\"\"\n","    ABBREVIATIONS = [\n","    \"Dr.\", \"Mr.\", \"Mrs.\", \"Ms.\", \"Lt.\", \"St.\", \"Capt.\", \"Col.\", \"Gen.\", \"Rev.\", \"Hon.\",\n","    \"Jr.\", \"Sr.\", \"Ph.D.\", \"M.D.\", \"B.A.\", \"M.A.\", \"D.D.S.\", \"J.D.\", \"M.B.A.\", \"C.P.A.\",\n","    \"a.m.\", \"p.m.\", \"etc.\", \"i.e.\", \"e.g.\", \"vs.\", \"et al.\", \"approx.\", \"circa\", \"fig.\",\n","    \"Jan.\", \"Feb.\", \"Mar.\", \"Apr.\", \"May.\", \"Jun.\", \"Jul.\", \"Aug.\", \"Sep.\", \"Oct.\", \"Nov.\", \"Dec.\",\n","    \"Prof.\", \"Assoc.\", \"Asst.\", \"Dept.\", \"Univ.\", \"Gov.\", \"Sen.\", \"Rep.\", \"Pres.\",\n","    \"Vol.\", \"No.\", \"Pt.\", \"Ch.\", \"Sec.\", \"Subsec.\", \"Para.\", \"Subpara.\", \"Cl.\", \"Subcl.\",\n","    \"Ex.\", \"Exs.\", \"Fig.\", \"Figs.\", \"Eq.\", \"Eqs.\", \"Thm.\", \"Thms.\", \"Prop.\", \"Props.\",\n","    \"Def.\", \"Defs.\", \"Cor.\", \"Cors.\", \"Lem.\", \"Lems.\", \"Proof.\", \"Proofs.\",\n","    \"min.\", \"hrs.\", \"sec.\", \"yr.\", \"yrs.\", \"mo.\", \"mos.\", \"wk.\", \"wks.\",\n","    \"ft.\", \"in.\", \"cm.\", \"mm.\", \"kg.\", \"g.\", \"oz.\", \"lb.\", \"lbs.\", \"ml.\", \"l.\",\n","    \"A.\", \"B.\", \"C.\", \"D.\", \"E.\", \"F.\", \"G.\", \"H.\", \"I.\", \"J.\", \"K.\", \"L.\", \"M.\",\n","    \"N.\", \"O.\", \"P.\", \"Q.\", \"R.\", \"S.\", \"T.\", \"U.\", \"V.\", \"W.\", \"X.\", \"Y.\", \"Z.\",\n","    \"1.\", \"2.\", \"4.\", \"4.\", \"5.\", \"6.\", \"7.\", \"8.\", \"9.\",\n","    \"10.\", \"20.\", \"30.\", \"40.\", \"50.\", \"60.\", \"70.\", \"80.\", \"90.\",\n","    \"11.\", \"12.\", \"13.\", \"14.\", \"15.\", \"16.\", \"17.\", \"18.\", \"19.\",\n","    \"21.\", \"22.\", \"23.\", \"24.\", \"25.\", \"26.\", \"27.\", \"28.\", \"29.\",\n","    \"31.\", \"32.\", \"33.\", \"34.\", \"35.\", \"36.\", \"37.\", \"38.\", \"39.\",\n","    \"41.\", \"42.\", \"43.\", \"44.\", \"45.\", \"46.\", \"47.\", \"48.\", \"49.\",\n","    \"51.\", \"52.\", \"53.\", \"54.\", \"55.\", \"56.\", \"57.\", \"58.\", \"59.\",\n","    \"61.\", \"62.\", \"63.\", \"64.\", \"65.\", \"66.\", \"67.\", \"68.\", \"69.\",\n","    \"71.\", \"72.\", \"73.\", \"74.\", \"75.\", \"76.\", \"77.\", \"78.\", \"79.\",\n","    \"81.\", \"82.\", \"83.\", \"84.\", \"85.\", \"86.\", \"87.\", \"88.\", \"89.\",\n","    \"91.\", \"92.\", \"93.\", \"94.\", \"95.\", \"96.\", \"97.\", \"98.\", \"99.\",\n","\n","    \"I.\", \"II.\", \"III.\", \"IV.\", \"V.\", \"VI.\", \"VII.\", \"VIII.\", \"IX.\", \"X.\",\n","    \"XI.\", \"XII.\", \"XIII.\", \"XIV.\", \"XV.\", \"XVI.\", \"XVII.\", \"XVIII.\", \"XIX.\", \"XX.\",\n","    \"i.\", \"ii.\", \"iii.\", \"iv.\", \"v.\", \"vi.\", \"vii.\", \"viii.\", \"ix.\", \"x.\",\n","    \"xi.\", \"xii.\", \"xiii.\", \"xiv.\", \"xv.\", \"xvi.\", \"xvii.\", \"xviii.\", \"xix.\", \"xx.\",\n","    \"a.\", \"b.\", \"c.\", \"d.\", \"e.\", \"f.\", \"g.\", \"h.\", \"i.\", \"j.\", \"k.\", \"l.\", \"m.\",\n","    \"n.\", \"o.\", \"p.\", \"q.\", \"r.\", \"s.\", \"t.\", \"u.\", \"v.\", \"w.\", \"x.\", \"y.\", \"z.\"\n","    ]\n","\n","\n","    # Construct a pattern to match abbreviations\n","    abbreviations_pattern = r\"|\".join(r\"\\b{}\\b\".format(re.escape(abbr)) for abbr in ABBREVIATIONS)\n","\n","    # This pattern attempts to split at sentence endings (.?!), including the punctuation with the preceding sentence\n","    # It uses a lookahead to keep the punctuation with the sentence\n","    # The negative lookahead (?!({abbreviations_pattern})\\s) excludes known abbreviations from being split\n","    sentence_end_regex = r'(?<=[.!?])\\s+(?=[A-Z])(?!({abbreviations_pattern})\\s)'.format(abbreviations_pattern=abbreviations_pattern)\n","\n","    split_sentences_and_punctuation_list = re.split(sentence_end_regex, text)\n","\n","    # Optionally, remove empty strings if they are not desired\n","    split_sentences_and_punctuation_list = [s for s in split_sentences_and_punctuation_list if s]\n","\n","    return split_sentences_and_punctuation_list\n","\n","\n","\n","# def split_sentences_and_punctuation(text):\n","#     \"\"\"Splits text into sentences, attempting to preserve punctuation and all text content.\n","\n","#     Args:\n","#         text (str): The input text.\n","\n","#     Returns:\n","#         list: A list of sentences with preserved punctuation.\n","#     \"\"\"\n","\n","#     # This pattern attempts to split at sentence endings (.?!), including the punctuation with the preceding sentence\n","#     # It uses a lookahead to keep the punctuation with the sentence\n","#     sentence_end_regex = r'(?<=[.!?])\\s+(?=[A-Z])'\n","\n","#     split_sentences_and_punctuation_list = re.split(sentence_end_regex, text)\n","\n","#     # Optionally, remove empty strings if they are not desired\n","#     split_sentences_and_punctuation_list = [s for s in split_sentences_and_punctuation_list if s]\n","\n","#     # print(\"split_sentences_and_punctuation_list\")\n","#     # print(split_sentences_and_punctuation_list)\n","\n","#     return split_sentences_and_punctuation_list\n","\n","\n","def recombine_punctuation(sentences):\n","    \"\"\"\n","    A helper function a that\n","    Recombines floating punctuation and\n","    creates a new list of sentences.\"\"\"\n","    recombined_sentences = []\n","    i = 0\n","\n","    while i < len(sentences) - 1:\n","        # print(i)\n","        # print(sentences[i-1])\n","        # print(sentences[i])\n","        # print(sentences[i+1])\n","\n","        sentence = sentences[i].strip()\n","        next_item = sentences[i + 1].strip()\n","\n","        # if next_item in \".?!\":\n","        if re.match(r\"[.?!]+\", next_item):\n","            recombined_sentences.append(sentence + next_item)\n","            i += 1  # Skip the punctuation since it's been combined\n","        else:\n","            recombined_sentences.append(sentence)\n","        i += 1\n","\n","    # Add the last sentence (if it exists)\n","    if sentences[-1]:\n","        recombined_sentences.append(sentences[-1].strip())\n","\n","    return recombined_sentences\n","\n","\n","def chunk_text(sentences, chunk_size, overlap_size=0):\n","    chunked_text = []\n","    current_chunk = \"\"\n","    overlap_text = \"\"\n","\n","    for sentence in sentences:\n","        # Case 1: Chunk + sentence easily fit\n","        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n","            current_chunk += sentence + \" \"\n","\n","        # Case 2: Sentence itself is too big\n","        elif len(sentence) > chunk_size:\n","            # Split long sentence (implement 'split_long_sentence' below)\n","            for sub_sentence in split_long_sentence(sentence, chunk_size):\n","                chunked_text.append(sub_sentence.strip())\n","\n","        # Case 3:  Chunk + sentence exceed limit, time to split\n","        else:\n","            chunked_text.append(current_chunk.strip())\n","            current_chunk = sentence + \" \"\n","\n","    # Handle final chunk\n","    if current_chunk:\n","        chunked_text.append(current_chunk.strip())\n","\n","    return chunked_text\n","\n","\n","def split_long_sentence(sentence, chunk_size):\n","    \"\"\"Splits a long sentence into chunks, aiming near the  chunk_size.\"\"\"\n","    words = sentence.split()\n","    chunks = []\n","    current_chunk = \"\"\n","\n","    for word in words:\n","        if len(current_chunk) + len(word) + 1 <= chunk_size:\n","            current_chunk += word + \" \"\n","        else:\n","            chunks.append(current_chunk.strip())\n","            current_chunk = word + \" \"\n","\n","    if current_chunk:\n","        chunks.append(current_chunk.strip())\n","\n","    return chunks\n","\n","\n","def check_for_not(chunk, window_size=25):\n","    \"\"\"Checks if 'not' is isolated near a potential cut.\n","\n","    Args:\n","        chunk (list): A list of sentences forming the chunk.\n","        window_size (int): The number of characters to consider on either side.\n","\n","    Returns:\n","        bool: True if 'not' is isolated, False otherwise.\n","    \"\"\"\n","\n","    joined_chunk = ' '.join(chunk)\n","    not_indices = [m.start() for m in re.finditer(r'\\bnot\\b', joined_chunk)]\n","\n","    for index in not_indices:\n","        start = max(0, index - window_size)\n","        end = min(len(joined_chunk), index + window_size)\n","        if not re.search(r'\\w', joined_chunk[start:end]):  # Check for surrounding words\n","            return True\n","\n","    return False\n","\n","\n","def make_chunk_list(text, chunk_size, this_epub_output_dir_path):\n","    split_sentences_list = split_sentences_and_punctuation(text)\n","    chunk_list = chunk_text(split_sentences_list, chunk_size)\n","\n","    for i in chunk_list:\n","        if not i:\n","            print_and_log(\"error None in chunk_list: make_chunk_list()\")\n","\n","    print_and_log(f\"len chunk list -> {len(chunk_list)}\", this_epub_output_dir_path)\n","\n","    return chunk_list\n","\n","\n","######\n","# Run\n","######\n","\"\"\"\n","1. add your epub files into the same current working directory as this script\n","2. run script\n","3. find the files in new folders per epub\n","\"\"\"\n","import glob\n","\n","# Get the current working directory.\n","cwd = os.getcwd()\n","\n","# Search for all EPUB files in the current working directory.\n","epub_files = glob.glob(os.path.join(cwd, \"*.epub\"))\n","\n","# Print the list of EPUB files.\n","print(epub_files)\n","\n","\n","####################\n","# run for each epub\n","####################\n","for this_epub_file in epub_files:\n","\n","    # set target epub to first epub doc listed as being in the cwd\n","    epub_file_path = this_epub_file\n","\n","    # make directory for this book\n","    this_epub_output_dir_path = epub_file_path[:-5] + \"_epub_folder\"\n","    print(this_epub_output_dir_path)\n","\n","    # Set the absolute path\n","    this_epub_output_dir_path = os.path.abspath(this_epub_output_dir_path)\n","\n","    # Create a directory for individual txt files\n","    if not os.path.exists(this_epub_output_dir_path):\n","        os.makedirs(this_epub_output_dir_path)\n","\n","    # json\n","    # output_jsonl_path = 'output.jsonl'\n","    output_jsonl_path = os.path.join(this_epub_output_dir_path, 'output.jsonl')\n","    output_json_dir = os.path.join(this_epub_output_dir_path, 'individual_jsons')  # Directory to store individual JSON files\n","    output_json_zip_dir = os.path.join(this_epub_output_dir_path, 'jsons_zip_archive')  # Directory to store individual JSON files\n","\n","    # txt\n","    output_whole_txt_path = os.path.join(this_epub_output_dir_path, 'whole.txt')\n","    output_txt_dir = os.path.join(this_epub_output_dir_path, 'individual_txt')  # Directory to store individual txt files\n","    output_txt_zip_dir = os.path.join(this_epub_output_dir_path, 'txt_zip_archive')  # Directory to store individual JSON files\n","\n","\n","    # chunks\n","    output_chunks_jsonl_path = os.path.join(this_epub_output_dir_path, 'chunks_jsonl_all.jsonl')  # Directory to store individual txt files\n","    output_chunks_dir = os.path.join(this_epub_output_dir_path, 'chunk_text_files')  # Directory to store individual txt files\n","    output_chunks_zip_dir = os.path.join(this_epub_output_dir_path, 'chunks_zip_archive')  # Directory to store individual JSON files\n","\n","    extract_text_from_epub(epub_file_path,\n","                           this_epub_output_dir_path,\n","                           output_jsonl_path,\n","                           output_json_dir,\n","                           output_whole_txt_path,\n","                           output_txt_dir,\n","                           output_chunks_jsonl_path,\n","                           output_chunks_dir,\n","                           max_chunk_size=500)\n","\n","\n","    # Call the zip function\n","    \"\"\"\n","    zip_folder(path_to_directory_to_zip, output_destination_zip_file_path)\n","    \"\"\"\n","    zip_folder(output_json_dir, output_json_zip_dir)\n","    zip_folder(output_txt_dir, output_txt_zip_dir)\n","    zip_folder(output_chunks_dir, output_chunks_zip_dir)\n","\n","\n","\n","######\n","# Run\n","######\n","\"\"\"\n","1. add your epub files into the same current working directory as this script\n","2. run script\n","3. find the files in new folders per epub\n","\"\"\"\n","import glob\n","\n","# Get the current working directory.\n","cwd = os.getcwd()\n","\n","# Search for all EPUB files in the current working directory.\n","txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n","\n","# Print the list of txt files.\n","print(txt_files)\n","\n","\n","####################\n","# run for each txt\n","####################\n","for this_txt_file in txt_files:\n","\n","    # set target txt to first txt doc listed as being in the cwd\n","    txt_file_path = this_txt_file\n","\n","    # make directory for this book\n","    this_txt_output_dir_path = txt_file_path[:-5] + \"_txt_folder\"\n","    print(this_txt_output_dir_path)\n","\n","    # Set the absolute path\n","    this_txt_output_dir_path = os.path.abspath(this_txt_output_dir_path)\n","\n","    # Create a directory for individual txt files\n","    if not os.path.exists(this_txt_output_dir_path):\n","        os.makedirs(this_txt_output_dir_path)\n","\n","    # json\n","    # output_jsonl_path = 'output.jsonl'\n","    output_jsonl_path = os.path.join(this_txt_output_dir_path, 'output.jsonl')\n","    output_json_dir = os.path.join(this_txt_output_dir_path, 'individual_jsons')  # Directory to store individual JSON files\n","    output_json_zip_dir = os.path.join(this_txt_output_dir_path, 'jsons_zip_archive')  # Directory to store individual JSON files\n","\n","    # txt\n","    output_whole_txt_path = os.path.join(this_txt_output_dir_path, 'whole.txt')\n","    output_txt_dir = os.path.join(this_txt_output_dir_path, 'individual_txt')  # Directory to store individual txt files\n","    output_txt_zip_dir = os.path.join(this_txt_output_dir_path, 'txt_zip_archive')  # Directory to store individual JSON files\n","\n","\n","    # chunks\n","    output_chunks_jsonl_path = os.path.join(this_txt_output_dir_path, 'chunks_jsonl_all.jsonl')  # Directory to store individual txt files\n","    output_chunks_dir = os.path.join(this_txt_output_dir_path, 'chunk_text_files')  # Directory to store individual txt files\n","    output_chunks_zip_dir = os.path.join(this_txt_output_dir_path, 'chunks_zip_archive')  # Directory to store individual JSON files\n","\n","    extract_text_from_txt(txt_file_path,\n","                           this_txt_output_dir_path,\n","                           output_jsonl_path,\n","                           output_json_dir,\n","                           output_whole_txt_path,\n","                           output_txt_dir,\n","                           output_chunks_jsonl_path,\n","                           output_chunks_dir,\n","                           max_chunk_size=500)\n","\n","\n","    # Call the zip function\n","    \"\"\"\n","    zip_folder(path_to_directory_to_zip, output_destination_zip_file_path)\n","    \"\"\"\n","    zip_folder(output_json_dir, output_json_zip_dir)\n","    zip_folder(output_txt_dir, output_txt_zip_dir)\n","    zip_folder(output_chunks_dir, output_chunks_zip_dir)\n"],"metadata":{"id":"ywO_m5TMLFlN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713308718998,"user_tz":240,"elapsed":161,"user":{"displayName":"On Off","userId":"17227537208113104618"}},"outputId":"0b4b56dd-86ec-47a1-e1ca-724b5c12e835"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","['/content/AI_Driven_Entrepreneur_2.txt']\n","/content/AI_Driven_Entrepreneur__txt_folder\n","len chunk list -> 345\n","Size Check, OK \\o/\n","Chunked: split into this many chunks-> 345\n","/content/AI_Driven_Entrepreneur_2.txt -> ok!\n","OK!\n"]}]},{"cell_type":"markdown","source":["# Loading dataset as huggingface data"],"metadata":{"id":"2aQLee-vtqZV"}},{"cell_type":"code","source":["from datasets import load_dataset, Features, Value\n","\n","# Define the features of your dataset\n","features = Features({\n","    'source_name': Value('string'),\n","    'text': Value('string')\n","})\n","\n","# Load the dataset with the defined features\n","dataset = load_dataset(\n","    \"json\",\n","    data_files=\"rustforrustaceans_epub_folder/chunks_jsonl_all.json\",  # Ensure this path is correct\n","    features=features\n",")\n","\n","print(dataset)\n"],"metadata":{"id":"zS3-jVw3tqgf"},"execution_count":null,"outputs":[]}]}