{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXnvGVW9DiMMjYCI0zwFQw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MRHm9NtbyBrz"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["\n","Based on:\n","\"Fine-Tune Your Own Tiny-Llama on Custom Dataset\"\n","by: Prompt Engineering\n","https://www.youtube.com/watch?v=OVqe6GTrDFM\n","the colab by: Prompt Engineering\n","https://colab.research.google.com/drive/12-5O6NvMdISOe0KVeHBrLkHMxMFqGp9Y?usp=sharing\n","\n","# article that inspired this projects\n","https://mychen76.medium.com/tinyllama-colorist-fine-tuned-with-color-dataset-8cd1cf7e5665\n","\n","https://github.com/minyang-chen/tinyllama_colorist/blob/main/Finetune_TinyLlama_with_colorist.ipynb\n","\n"],"metadata":{"id":"fYuo4aiBQXiP"}},{"cell_type":"markdown","source":["# TODO\n","It appears that a 'text' field is added and this field\n","is used by the lora-layer trainer...\n","\n","\n","Q: could a jsonl with a text field also be used?\n"],"metadata":{"id":"lFfd5P8MgUmO"}},{"cell_type":"code","source":["#########################################################\n","# This code automaticaly finds and processes epub books\n","# esspecially for RAG document ingestion processing\n","#########################################################\n","\"\"\"\n","input -> one or more epub files\n","output -> txt and json files that contain the text from sections of the book\n","          as well as smart-chunked segments made to your size specs\n","          e.g. a max of 500 characters, which contain whole sentences.\n","          Chunks do not cut words or sentences in half.\n","\n","# Set of Results, saved in a file per epub doc:\n","1. One .jsonl file\n","2. (Plural) Individual .json files in a folder\n","3. One .txt text file of the whole epub\n","4. (plural) Individual .txt files from separate parts of epub\n","5. chunks as one .jsonl file\n","6. (Plural) chunks under specified character length as separate text\n","Future Feature:\n","7. Chunk-Metadata (for model training, for DB-retrieval, etc.)\n","\"\"\"\n","\n","import zipfile\n","import xml.etree.ElementTree as ET\n","from bs4 import BeautifulSoup\n","import json\n","import os\n","import shutil\n","\n","\n","def get_ordered_html_files(opf_content):\n","    \"\"\"\n","    Parses the content.opf file to determine the reading order of HTML files in the EPUB.\n","\n","    The function reads the 'content.opf' file, which contains metadata about the EPUB's structure.\n","    It identifies the 'spine' element, which lists the reading order of the content documents,\n","    and the 'manifest' element, which provides the location of these documents.\n","    The function returns a list of HTML file paths in the order they should be read.\n","\n","    Args:\n","    opf_content (str): A string representation of the content.opf file.\n","\n","    Returns:\n","    list: An ordered list of HTML file paths as specified in the EPUB's spine.\n","    \"\"\"\n","\n","    # Parse the content.opf XML content\n","    tree = ET.ElementTree(ET.fromstring(opf_content))\n","    root = tree.getroot()\n","\n","    # Define the namespace for the OPF package file\n","    ns = {'opf': 'http://www.idpf.org/2007/opf'}\n","\n","    # Find the spine element which indicates the order of the content documents\n","    spine = root.find('opf:spine', ns)\n","    itemrefs = spine.findall('opf:itemref', ns)\n","\n","    # Extract the id references for each item in the spine\n","    item_ids = [itemref.get('idref') for itemref in itemrefs]\n","\n","    # Find the manifest element which lists all the content documents\n","    manifest = root.find('opf:manifest', ns)\n","    items = manifest.findall('opf:item', ns)\n","\n","    # Create a dictionary mapping item IDs to their corresponding file paths\n","    html_files = {item.get('id'): item.get('href') for item in items if item.get('media-type') == 'application/xhtml+xml'}\n","\n","    # Generate an ordered list of HTML files based on the spine order\n","    ordered_html_files = [html_files[item_id] for item_id in item_ids if item_id in html_files]\n","\n","    return ordered_html_files\n","\n","\n","def extract_text_from_html(html_content, this_epub_output_dir_path):\n","    \"\"\"\n","    Extracts and returns text from an HTML content.\n","    \"\"\"\n","    # print(\"HTML Content before BeautifulSoup Parsing:\\n\", html_content[:500])  # Print first 500 characters of HTML\n","    print_and_log(f\"\\nlen(HTML Content before BeautifulSoup Parsing) -> {len(html_content)}\", this_epub_output_dir_path)  # Print first 500 characters of HTML\n","\n","\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    parsed_text = soup.get_text()\n","    # print(\"Extracted Text:\\n\", parsed_text[:500])  # Print first 500 characters of extracted text\n","    print_and_log(f\"len(Extracted Text) -> {len(parsed_text)}\", this_epub_output_dir_path)  # Print first 500 characters of extracted text\n","\n","    return parsed_text\n","\n","def fix_text_formatting(text):\n","    \"\"\"Replaces the Unicode right single quotation mark with a standard apostrophe.\"\"\"\n","    return text.replace(\"\\u2019\", \"'\")\n","\n","\n","def check_len_chunks_in_list(chunks_list, max_chunk_size, this_epub_output_dir_path):\n","\n","    size_flag_ok = True\n","\n","    for index, this_chunk in enumerate(chunks_list):\n","\n","        # get size of chunk\n","        this_length = len( this_chunk )\n","\n","        # check size against user-input max size\n","        if this_length > max_chunk_size:\n","            print_and_log( this_length, this_epub_output_dir_path )\n","            print_and_log( f\"\"\"\n","            Warning: chunk over max size.\n","            This chunk size: {this_chunk}.\n","            Max size: {max_chunk_size}\n","            \"\"\" )\n","            print_and_log( f\"This chunk: {this_chunk}\", this_epub_output_dir_path )\n","            size_flag_ok = False\n","\n","    # report and log\n","    if size_flag_ok:\n","        print_and_log( \"Size Check, OK \\o/\", this_epub_output_dir_path )\n","    else:\n","        print_and_log( \"WARNING: Size Check Failed!\", this_epub_output_dir_path )\n","\n","\n","\n","def save_individual_chunks(chunks_list, output_chunks_dir, chunk_source_name):\n","\n","    for index, this_chunk in enumerate(chunks_list):\n","        chunk_name = f\"{chunk_source_name}_{index}.txt\"\n","\n","        # Save individual txt files\n","        individual_chunk_path = os.path.join(output_chunks_dir, chunk_name)\n","        with open(individual_chunk_path, 'w') as f:\n","            f.write(this_chunk)\n","            # print('chunk writen', individual_chunk_path)\n","\n","    return len(chunks_list)\n","\n","\n","def append_chunks_to_jsonl(chunks_list, output_chunks_jsonl_path, chunk_source_name):\n","    \"\"\"Appends chunks of text to a .jsonl file, each chunk as a JSON object.\n","\n","    Args:\n","        chunks_list (list): List of text chunks to be appended.\n","        output_jsonl_path (str): The output file path for the .jsonl file.\n","        chunk_source_name (str): Base name for each chunk, used in the 'source_name' field.\n","\n","    Returns:\n","        int: The number of chunks appended.\n","    \"\"\"\n","\n","    with open(output_chunks_jsonl_path, 'a') as f:  # Open file in append mode\n","        for index, this_chunk in enumerate(chunks_list):\n","            # Construct a JSON object for the chunk\n","            chunk_data = {\n","                \"source_name\": f\"{chunk_source_name}_{index}\",\n","                \"text\": this_chunk\n","            }\n","\n","            # Convert the chunk data to a JSON string and append it to the file with a newline\n","            f.write(json.dumps(chunk_data) + '\\n')\n","\n","    return len(chunks_list)\n","\n","\n","def print_and_log(input_text, this_epub_output_dir_path):\n","    # check if input is a string, if not...make it a string!\n","    if not isinstance(input_text, str):\n","        input_text = str(input_text)\n","\n","    # print to terminal\n","    print(input_text)\n","\n","    # log file path is...\n","    log_file_path = os.path.join(this_epub_output_dir_path, \"log.txt\")\n","\n","    # log: Write/Append to a log.txt file\n","    with open(log_file_path, 'a') as f:\n","        f.write(input_text + '\\n\\n')\n","\n","\n","def extract_text_from_epub(epub_file_path, this_epub_output_dir_path, output_jsonl_path, output_json_dir, output_whole_txt_path, output_txt_dir, output_chunks_jsonl_path, output_chunks_dir, max_chunk_size=500):\n","    with zipfile.ZipFile(epub_file_path, 'r') as epub:\n","        print_and_log(f\"EPUB Contents: -> {epub.namelist()}\", this_epub_output_dir_path)\n","\n","        ###################\n","        # Make Directories\n","        ###################\n","\n","        # Create a directory for individual JSON files\n","        if not os.path.exists(output_json_dir):\n","            os.makedirs(output_json_dir)\n","\n","        # Create a directory for individual txt files\n","        if not os.path.exists(output_txt_dir):\n","            os.makedirs(output_txt_dir)\n","\n","        # Create a directory for chunks output_chunks_dir\n","        if not os.path.exists(output_chunks_dir):\n","            os.makedirs(output_chunks_dir)\n","\n","\n","        ##################################\n","        # Get & Read html files from epub\n","        ##################################\n","        # find opf file\n","        opf_file = [f for f in epub.namelist() if 'content.opf' in f][0]\n","\n","        # read opf file\n","        opf_content = epub.read(opf_file).decode('utf-8')\n","\n","        # get ordered HTML files\n","        ordered_html_files_list = get_ordered_html_files(opf_content)\n","\n","\n","        ############################################\n","        # Read and extract text from each HTML file\n","        ############################################\n","\n","        # iterate through html files\n","        for html_file in ordered_html_files_list:\n","            full_path = os.path.join(os.path.dirname(opf_file), html_file)\n","            if full_path in epub.namelist():\n","                html_content = epub.read(full_path).decode('utf-8')\n","\n","                #########################\n","                # extract text from epub\n","                #########################\n","                raw_text = extract_text_from_html(html_content, this_epub_output_dir_path)\n","                print_and_log(f\"len(text for json)-> {len(raw_text)}\", this_epub_output_dir_path)\n","\n","                # fix text formatting\n","                text = fix_text_formatting(raw_text)\n","\n","\n","                #################\n","                # .json & .jsonl\n","                #################\n","\n","                # Write/Append to a single JSONL file\n","                with open(output_jsonl_path, 'a') as f:\n","                    json_record = json.dumps({'text': text.strip()})\n","                    f.write(json_record + '\\n')\n","\n","                # Save individual JSON file\n","                individual_json_path = os.path.join(output_json_dir, f\"{os.path.splitext(html_file)[0]}.json\")\n","                with open(individual_json_path, 'w') as f:\n","                    json.dump({'text': text.strip()}, f, indent=4)\n","\n","                #######\n","                # .txt\n","                #######\n","\n","                # Write/Append to a single text .txt file\n","                with open(output_whole_txt_path, 'a') as f:\n","                    f.write(text + '\\n\\n')\n","\n","                # Save individual txt files\n","                individual_txt_path = os.path.join(output_txt_dir, f\"{os.path.splitext(html_file)[0]}.txt\")\n","                with open(individual_txt_path, 'w') as f:\n","                    f.write(text)\n","\n","                #########\n","                # chunks\n","                #########\n","\n","                chunks_list = make_chunk_list(text, max_chunk_size, this_epub_output_dir_path)\n","\n","                chunk_source_name = os.path.splitext(html_file)[0]\n","\n","                # check sizes\n","                check_len_chunks_in_list(chunks_list, max_chunk_size, this_epub_output_dir_path)\n","\n","                number_of_chunks = save_individual_chunks(chunks_list, output_chunks_dir, chunk_source_name)\n","                print_and_log(f\"Chunked: split into this many chunks-> {number_of_chunks}\", this_epub_output_dir_path)\n","\n","                append_chunks_to_jsonl(chunks_list, output_chunks_jsonl_path, chunk_source_name)\n","\n","                print_and_log(f\"{html_file} -> ok!\", this_epub_output_dir_path)\n","\n","\n","            else:  # File Not Found\n","                print_and_log(f\"Warning: File {full_path} not found in the archive.\")\n","\n","\n","def zip_folder(path_to_directory_to_zip, output_destination_zip_file_path):\n","    \"\"\"Creates a zip archive of a specified folder.\n","\n","    Args:\n","        path_to_directory_to_zip (str): The path to the folder to be zipped.\n","        output_destination_zip_file_path (str): The desired name and path of the output zip file.\n","    \"\"\"\n","    # # Specify the folder you want to zip\n","    # path_to_directory_to_zip = \"individual_jsons\"\n","\n","    # # Specify the desired output zip file name (e.g., 'jsons_archive.zip')\n","    # output_destination_zip_file_path = \"jsons_archive_zip\"\n","\n","    shutil.make_archive(output_destination_zip_file_path, 'zip', path_to_directory_to_zip)\n","\n","\n","###########\n","# Chunking\n","############\n","\n","import re\n","\n","def split_sentences_and_punctuation(text):\n","    \"\"\"Splits text into sentences, attempting to preserve punctuation and all text content.\n","\n","    Args:\n","        text (str): The input text.\n","\n","    Returns:\n","        list: A list of sentences with preserved punctuation.\n","    \"\"\"\n","\n","    # This pattern attempts to split at sentence endings (.?!), including the punctuation with the preceding sentence\n","    # It uses a lookahead to keep the punctuation with the sentence\n","    sentence_end_regex = r'(?<=[.!?])\\s+(?=[A-Z])'\n","\n","    split_sentences_and_punctuation_list = re.split(sentence_end_regex, text)\n","\n","    # Optionally, remove empty strings if they are not desired\n","    split_sentences_and_punctuation_list = [s for s in split_sentences_and_punctuation_list if s]\n","\n","    # print(\"split_sentences_and_punctuation_list\")\n","    # print(split_sentences_and_punctuation_list)\n","\n","    return split_sentences_and_punctuation_list\n","\n","\n","def recombine_punctuation(sentences):\n","    \"\"\"\n","    A helper function a that\n","    Recombines floating punctuation and\n","    creates a new list of sentences.\"\"\"\n","    recombined_sentences = []\n","    i = 0\n","\n","    while i < len(sentences) - 1:\n","        # print(i)\n","        # print(sentences[i-1])\n","        # print(sentences[i])\n","        # print(sentences[i+1])\n","\n","        sentence = sentences[i].strip()\n","        next_item = sentences[i + 1].strip()\n","\n","        # if next_item in \".?!\":\n","        if re.match(r\"[.?!]+\", next_item):\n","            recombined_sentences.append(sentence + next_item)\n","            i += 1  # Skip the punctuation since it's been combined\n","        else:\n","            recombined_sentences.append(sentence)\n","        i += 1\n","\n","    # Add the last sentence (if it exists)\n","    if sentences[-1]:\n","        recombined_sentences.append(sentences[-1].strip())\n","\n","    return recombined_sentences\n","\n","\n","def chunk_text(sentences, chunk_size):\n","    chunked_text = []\n","    current_chunk = \"\"\n","\n","    for sentence in sentences:\n","        # Case 1: Chunk + sentence easily fit\n","        if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n","            current_chunk += sentence + \" \"\n","\n","        # Case 2: Sentence itself is too big\n","        elif len(sentence) > chunk_size:\n","            # Split long sentence (implement 'split_long_sentence' below)\n","            for sub_sentence in split_long_sentence(sentence, chunk_size):\n","                chunked_text.append(sub_sentence.strip())\n","\n","        # Case 3:  Chunk + sentence exceed limit, time to split\n","        else:\n","            chunked_text.append(current_chunk.strip())\n","            current_chunk = sentence + \" \"\n","\n","    # Handle final chunk\n","    if current_chunk:\n","        chunked_text.append(current_chunk.strip())\n","\n","    return chunked_text\n","\n","\n","def split_long_sentence(sentence, chunk_size):\n","    \"\"\"Splits a long sentence into chunks, aiming near the  chunk_size.\"\"\"\n","    words = sentence.split()\n","    chunks = []\n","    current_chunk = \"\"\n","\n","    for word in words:\n","        if len(current_chunk) + len(word) + 1 <= chunk_size:\n","            current_chunk += word + \" \"\n","        else:\n","            chunks.append(current_chunk.strip())\n","            current_chunk = word + \" \"\n","\n","    if current_chunk:\n","        chunks.append(current_chunk.strip())\n","\n","    return chunks\n","\n","\n","def check_for_not(chunk, window_size=25):\n","    \"\"\"Checks if 'not' is isolated near a potential cut.\n","\n","    Args:\n","        chunk (list): A list of sentences forming the chunk.\n","        window_size (int): The number of characters to consider on either side.\n","\n","    Returns:\n","        bool: True if 'not' is isolated, False otherwise.\n","    \"\"\"\n","\n","    joined_chunk = ' '.join(chunk)\n","    not_indices = [m.start() for m in re.finditer(r'\\bnot\\b', joined_chunk)]\n","\n","    for index in not_indices:\n","        start = max(0, index - window_size)\n","        end = min(len(joined_chunk), index + window_size)\n","        if not re.search(r'\\w', joined_chunk[start:end]):  # Check for surrounding words\n","            return True\n","\n","    return False\n","\n","\n","def make_chunk_list(text, chunk_size, this_epub_output_dir_path):\n","    split_sentences_list = split_sentences_and_punctuation(text)\n","    chunk_list = chunk_text(split_sentences_list, chunk_size)\n","\n","    for i in chunk_list:\n","        if not i:\n","            print_and_log(\"error None in chunk_list: make_chunk_list()\")\n","\n","    print_and_log(f\"len chunk list -> {len(chunk_list)}\", this_epub_output_dir_path)\n","\n","    return chunk_list\n","\n","\n","######\n","# Run\n","######\n","\"\"\"\n","1. add your epub files into the same current working directory as this script\n","2. run script\n","3. find the files in new folders per epub\n","\"\"\"\n","import glob\n","\n","# Get the current working directory.\n","cwd = os.getcwd()\n","\n","# Search for all EPUB files in the current working directory.\n","epub_files = glob.glob(os.path.join(cwd, \"*.epub\"))\n","\n","# Print the list of EPUB files.\n","print(epub_files)\n","\n","\n","####################\n","# run for each epub\n","####################\n","for this_epub_file in epub_files:\n","\n","    # set target epub to first epub doc listed as being in the cwd\n","    epub_file_path = this_epub_file\n","\n","    # make directory for this book\n","    this_epub_output_dir_path = epub_file_path[:-5] + \"_epub_folder\"\n","    print(this_epub_output_dir_path)\n","\n","    # Set the absolute path\n","    this_epub_output_dir_path = os.path.abspath(this_epub_output_dir_path)\n","\n","    # Create a directory for individual txt files\n","    if not os.path.exists(this_epub_output_dir_path):\n","        os.makedirs(this_epub_output_dir_path)\n","\n","    # json\n","    # output_jsonl_path = 'output.jsonl'\n","    output_jsonl_path = os.path.join(this_epub_output_dir_path, 'output.jsonl')\n","    output_json_dir = os.path.join(this_epub_output_dir_path, 'individual_jsons')  # Directory to store individual JSON files\n","    output_json_zip_dir = os.path.join(this_epub_output_dir_path, 'jsons_zip_archive')  # Directory to store individual JSON files\n","\n","    # txt\n","    output_whole_txt_path = os.path.join(this_epub_output_dir_path, 'whole.txt')\n","    output_txt_dir = os.path.join(this_epub_output_dir_path, 'individual_txt')  # Directory to store individual txt files\n","    output_txt_zip_dir = os.path.join(this_epub_output_dir_path, 'txt_zip_archive')  # Directory to store individual JSON files\n","\n","\n","    # chunks\n","    output_chunks_jsonl_path = os.path.join(this_epub_output_dir_path, 'chunks_jsonl_all.jsonl')  # Directory to store individual txt files\n","    output_chunks_dir = os.path.join(this_epub_output_dir_path, 'chunk_text_files')  # Directory to store individual txt files\n","    output_chunks_zip_dir = os.path.join(this_epub_output_dir_path, 'chunks_zip_archive')  # Directory to store individual JSON files\n","\n","    extract_text_from_epub(epub_file_path,\n","                           this_epub_output_dir_path,\n","                           output_jsonl_path,\n","                           output_json_dir,\n","                           output_whole_txt_path,\n","                           output_txt_dir,\n","                           output_chunks_jsonl_path,\n","                           output_chunks_dir,\n","                           max_chunk_size=500)\n","\n","\n","    # Call the zip function\n","    \"\"\"\n","    zip_folder(path_to_directory_to_zip, output_destination_zip_file_path)\n","    \"\"\"\n","    zip_folder(output_json_dir, output_json_zip_dir)\n","    zip_folder(output_txt_dir, output_txt_zip_dir)\n","    zip_folder(output_chunks_dir, output_chunks_zip_dir)\n","\n"],"metadata":{"id":"ys0dnhpkhDL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNsMhNS1tIyy","outputId":"095a1cfe-37e4-4213-b6fd-171f7d6ce64b","executionInfo":{"status":"ok","timestamp":1707694733204,"user_tz":300,"elapsed":19104,"user":{"displayName":"On Off","userId":"17227537208113104618"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.8.2-py3-none-any.whl (183 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting trl\n","  Downloading trl-0.7.10-py3-none-any.whl (150 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Collecting datasets (from trl)\n","  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.7.2-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n","  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n","Collecting pyarrow>=12.0.0 (from datasets->trl)\n","  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->trl)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n","Collecting multiprocess (from datasets->trl)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n","Installing collected packages: shtab, pyarrow, docstring-parser, dill, multiprocess, bitsandbytes, tyro, accelerate, datasets, trl, peft\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 10.0.1\n","    Uninstalling pyarrow-10.0.1:\n","      Successfully uninstalled pyarrow-10.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.27.0 bitsandbytes-0.42.0 datasets-2.17.0 dill-0.3.8 docstring-parser-0.15 multiprocess-0.70.16 peft-0.8.2 pyarrow-15.0.0 shtab-1.6.5 trl-0.7.10 tyro-0.7.2\n"]}],"source":["!pip install accelerate peft bitsandbytes transformers trl"]},{"cell_type":"code","source":["import torch\n","from datasets import load_dataset, Dataset\n","from peft import LoraConfig, AutoPeftModelForCausalLM\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n","from trl import SFTTrainer\n","import os"],"metadata":{"id":"8mnl7YgdtxAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset=\"burkelibbey/colors\"\n","model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","output_model=\"tinyllama-rust-v1\""],"metadata":{"id":"4Ho3JJDLt6Pa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data preparation"],"metadata":{"id":"GnimXYnRvR9C"}},{"cell_type":"code","source":["# \"\"\"\n","# Original format from parent project\n","# \"\"\"\n","\n","# # we need to reformat the data in the ChatML format.\n","\n","# def formatted_train(input,response)->str:\n","#     return f\"<|im_start|>user\\n{input}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\""],"metadata":{"id":"_t4p54RquJIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def prepare_train_data(data_id):\n","#     #\n","#     data = load_dataset(data_id, split=\"train\")\n","\n","\n","#     data_df = data.to_pandas()\n","#     data_df[\"text\"] = data_df[[\"description\", \"color\"]].apply(lambda x: \"<|im_start|>user\\n\" + x[\"description\"] + \" <|im_end|>\\n<|im_start|>assistant\\n\" + x[\"color\"] + \"<|im_end|>\\n\", axis=1)\n","#     data = Dataset.from_pandas(data_df)\n","#     return data\n","\n","# data = prepare_train_data(dataset)"],"metadata":{"id":"yx7sBgDNdoJ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # name of color dataset to be downloaded by huggingface\n","# dataset_id=\"burkelibbey/colors\"\n","\n","# # download dataset: <class 'datasets.arrow_dataset.Dataset'>\n","# data = load_dataset(dataset_id, split=\"train\")\n","\n","# # inspection\n","# print( type(data) )\n","# print( data )\n","\n","# # convert to df (dataframe)\n","# data_df = data.to_pandas()\n","\n","# # function to convert dataframe to user: assistant: format\n","# def prepare_train_data(data_df):\n","#     data_df[\"text\"] = data_df[[\"description\", \"color\"]].apply(lambda x: \"<|im_start|>user\\n\" + x[\"description\"] + \" <|im_end|>\\n<|im_start|>assistant\\n\" + x[\"color\"] + \"<|im_end|>\\n\", axis=1)\n","#     data = Dataset.from_pandas(data_df)\n","#     return data\n","\n","# # structure dataset\n","# data = prepare_train_data(data_df)\n","\n","# print(len(data))\n","# print(data[0])"],"metadata":{"id":"H72S-7eiZFxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fSOcQx3BjJCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading from json1 test"],"metadata":{"id":"iMx4zAOAjJpQ"}},{"cell_type":"code","source":["# # load jsonl file as dict:\n","# import json\n","\n","# import json\n","\n","# data_dict = {}\n","# with open('rustforrustaceans_epub_folder/chunks_jsonl_all.json', 'r') as f:\n","#     for line in f:\n","#         data = json.loads(line)\n","#         data_dict.update(data)\n","\n","# print(len(data_dict))\n","# data_dict\n","\n","\n","# # Use the 'data' list which contains dictionaries\n","\n","# # dataset = load_dataset(\"namespace/your_dataset_name\", data_files=data_dict)"],"metadata":{"id":"Xfp2o2TqjHUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Adjust the path to your specific file\n","# file_path = 'rustforrustaceans_epub_folder/chunks_jsonl_all.json'\n","\n","# # Open and read the first few lines to inspect the structure\n","# with open(file_path, 'r') as file:\n","#     for i, line in enumerate(file):\n","#         print(json.loads(line))\n","#         if i >= 2:  # Adjust to read more lines as needed\n","#             break"],"metadata":{"id":"4SGY5kwBsNAJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://huggingface.co/docs/datasets/loading"],"metadata":{"id":"lARt1Ub8mfPD"}},{"cell_type":"code","source":["# from datasets import load_dataset\n","# dataset = load_dataset(\"json\", data_files=\"rustforrustaceans_epub_folder/chunks_jsonl_all.json\")\n"],"metadata":{"id":"kXu4TgOil-zT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset, Features, Value\n","\n","# Define the features of your dataset\n","features = Features({\n","    'source_name': Value('string'),\n","    'text': Value('string')\n","})\n","\n","# Load the dataset with the defined features\n","dataset = load_dataset(\n","    \"json\",\n","    data_files=\"rustforrustaceans_epub_folder/chunks_jsonl_all.json\",  # Ensure this path is correct\n","    features=features\n",")\n","\n","print(dataset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8X4mHsHs4Rn","executionInfo":{"status":"ok","timestamp":1707697852962,"user_tz":300,"elapsed":296,"user":{"displayName":"On Off","userId":"17227537208113104618"}},"outputId":"5811fb59-c5fb-4560-b4be-827b6c71e9ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['source_name', 'text'],\n","        num_rows: 1658\n","    })\n","})\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the entire dataset\n","dataset = load_dataset(\"json\", data_files=\"rustforrustaceans_epub_folder/chunks_jsonl_all.json\", split=\"train\")\n","\n","# # Manually split the dataset\n","# train_test_split = dataset[\"train\"].train_test_split(test_size=0.2)\n","# # Now, train_test_split has a 'train' split and a 'test' split\n","\n","# # If you want a validation split as well, you can further split the 'test' split\n","# test_val_split = train_test_split[\"test\"].train_test_split(test_size=0.5)\n","# # Now, test_val_split has a 'train' split (to be used as test) and a 'test' split (to be used as validation)\n","\n","# # Finally, you can rename the splits for clarity if needed\n","# final_splits = {\n","#     \"train\": train_test_split[\"train\"],\n","#     \"test\": test_val_split[\"train\"],\n","#     \"validation\": test_val_split[\"test\"],\n","# }\n","\n","print(final_splits)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4d-_V_3v7C_","executionInfo":{"status":"ok","timestamp":1707698754142,"user_tz":300,"elapsed":365,"user":{"displayName":"On Off","userId":"17227537208113104618"}},"outputId":"4130a3c8-dec1-4f66-9c47-5365f3ee2402"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': Dataset({\n","    features: ['source_name', 'text'],\n","    num_rows: 1326\n","}), 'test': Dataset({\n","    features: ['source_name', 'text'],\n","    num_rows: 166\n","}), 'validation': Dataset({\n","    features: ['source_name', 'text'],\n","    num_rows: 166\n","})}\n"]}]},{"cell_type":"code","source":["# print(dataset[\"train\"][0:5])\n","# print(len(dataset[\"train\"]))\n","# # print(len(dataset[\"test\"]))"],"metadata":{"id":"k3DMPpbMu6su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQK3Mg2TxuAQ","executionInfo":{"status":"ok","timestamp":1707698754142,"user_tz":300,"elapsed":6,"user":{"displayName":"On Off","userId":"17227537208113104618"}},"outputId":"76b3c4bd-3465-4c56-b2ea-98424971fbbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['source_name', 'text'],\n","    num_rows: 1658\n","})\n"]}]},{"cell_type":"code","source":["# # inspection\n","# print(type(data))\n","# # print(data)\n","# print(\"\\nlength: \", len(data))"],"metadata":{"id":"zPzJWidQvHXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model the Model (not the base version)"],"metadata":{"id":"Mnej4AYwvawA"}},{"cell_type":"code","source":["def get_model_and_tokenizer(model_id):\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n","    )\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id, quantization_config=bnb_config, device_map=\"auto\"\n","    )\n","\n","    model.config.use_cache=False\n","\n","    model.config.pretraining_tp=1\n","\n","    return model, tokenizer"],"metadata":{"id":"WrniDfzlvNd7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install -i https://test.pypi.org/simple/bitsandbytes"],"metadata":{"id":"BaD23-Fev7sK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, tokenizer = get_model_and_tokenizer(model_id)"],"metadata":{"id":"ogWCiBBzvk1C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Setting up the LoRA"],"metadata":{"id":"d2hOfQBZw_YX"}},{"cell_type":"code","source":["peft_config = LoraConfig(\n","        r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n","    )"],"metadata":{"id":"JAGakLbmvtj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training_arguments = TrainingArguments(\n","#         output_dir=output_model,\n","#         per_device_train_batch_size=16,\n","#         gradient_accumulation_steps=4,\n","#         optim=\"paged_adamw_32bit\",\n","#         learning_rate=2e-4,\n","#         lr_scheduler_type=\"cosine\",\n","#         save_strategy=\"epoch\",\n","#         logging_steps=10,\n","#         num_train_epochs=3,\n","#         max_steps=250,\n","#         fp16=True,\n","#         # push_to_hub=True\n","#     )"],"metadata":{"id":"Suz5-CbVxDGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_arguments = TrainingArguments(\n","        output_dir=output_model,\n","        per_device_train_batch_size=16,\n","        gradient_accumulation_steps=4,\n","        optim=\"paged_adamw_32bit\",\n","        learning_rate=2e-4,\n","        lr_scheduler_type=\"cosine\",\n","        save_strategy=\"epoch\",\n","        logging_steps=10,\n","        num_train_epochs=3,\n","        max_steps=250,\n","        fp16=True,\n","        # push_to_hub=True\n","    )"],"metadata":{"id":"6X6YzMYIytl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","        model=model,\n","        train_dataset=dataset,\n","        peft_config=peft_config,\n","        dataset_text_field=\"text\",\n","        args=training_arguments,\n","        tokenizer=tokenizer,\n","        packing=False,\n","        max_seq_length=1024\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7e4f53d858644c6ea07c3e335bfd6ee4","80347a3d48534b75aa4f49709831ebf7","78e3be425843451baaeff6a55329815c","94865552590b488f86e3fc2a921b3d3f","85a7ac78f0b7461982f1bec4c73af71e","ab5cad83f16a4f03914b7dea7a94ac23","fc4978abcb5047679fee5b23108695de","f3f0b748747446daa1cf6ca7d18ffe01","f569bce4685a44e39a86a497a32fd3a1","b63688433b0943169c61ac9851a29d64","b09cf15bb6394207b5ebbb1e6fef68e9"]},"id":"g7KtgzA8xGVs","outputId":"d061c4a9-7678-4d24-fde5-259fed907182","executionInfo":{"status":"ok","timestamp":1707698758160,"user_tz":300,"elapsed":863,"user":{"displayName":"On Off","userId":"17227537208113104618"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1658 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4f53d858644c6ea07c3e335bfd6ee4"}},"metadata":{}}]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"nXOXGKK0xU6X","outputId":"d3f494b9-9c05-4080-cfcb-8aecd79196b7","executionInfo":{"status":"error","timestamp":1707698764059,"user_tz":300,"elapsed":5904,"user":{"displayName":"On Off","userId":"17227537208113104618"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 29.06 MiB is free. Process 10952 has 14.72 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 623.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             return self.base_model(\n\u001b[0m\u001b[1;32m   1084\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    920\u001b[0m                 )\n\u001b[1;32m    921\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_SingleLevelFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 29.06 MiB is free. Process 10952 has 14.72 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 623.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["### Merging the LoRA with the base model"],"metadata":{"id":"Z1DJz6Zr4Jke"}},{"cell_type":"code","source":["from peft import AutoPeftModelForCausalLM, PeftModel\n","from transformers import AutoModelForCausalLM\n","import torch\n","import os\n","\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, load_in_8bit=False,\n","                                             device_map=\"auto\",\n","                                             trust_remote_code=True)\n","\n","model_path = \"/content/tinyllama-colorist-v1/checkpoint-250\"\n","\n","peft_model = PeftModel.from_pretrained(model, model_path, from_transformers=True, device_map=\"auto\")\n","\n","model = peft_model.merge_and_unload()"],"metadata":{"id":"V6QKygPu4E91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"SU2EIsuI4v50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Inference from the LLM"],"metadata":{"id":"n1nOANDaynhU"}},{"cell_type":"code","source":["from transformers import GenerationConfig\n","from time import perf_counter\n","\n","def generate_response(user_input):\n","\n","  prompt = formatted_prompt(user_input)\n","\n","  inputs = tokenizer([prompt], return_tensors=\"pt\")\n","  generation_config = GenerationConfig(penalty_alpha=0.6,do_sample = True,\n","      top_k=5,temperature=0.5,repetition_penalty=1.2,\n","      max_new_tokens=12,pad_token_id=tokenizer.eos_token_id\n","  )\n","  start_time = perf_counter()\n","\n","  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n","\n","  outputs = model.generate(**inputs, generation_config=generation_config)\n","  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","  output_time = perf_counter() - start_time\n","  print(f\"Time taken for inference: {round(output_time,2)} seconds\")"],"metadata":{"id":"mWKVO_5j5pSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def formatted_prompt(question)-> str:\n","    return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\""],"metadata":{"id":"yIT0IuT9HtEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_color_space(hex_color):\n","    def hex_to_rgb(hex_color):\n","        hex_color = hex_color.lstrip('#')\n","        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n","    r, g, b = hex_to_rgb(hex_color)\n","    print(f'{hex_color}: \\033[48;2;{r};{g};{b}m           \\033[0m')"],"metadata":{"id":"cbRpHaNwHthY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_response(user_input='Light Orange color')"],"metadata":{"id":"_FeisorS5pZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_color_space('#f0901a ')\n"],"metadata":{"id":"QvuqsFit5pcU"},"execution_count":null,"outputs":[]}]}