{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F1ISfYrmc7Re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set of Results:\n",
        "1. One jsonl file\n",
        "2. Individual json files in a folder\n",
        "3. One .txt text file of the whole epub\n",
        "4. Individual txt files from separate parts of epub"
      ],
      "metadata": {
        "id": "WB6jOKNFGEQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "# This code automaticaly finds and processes epub books\n",
        "# esspecially for RAG document ingestion processing\n",
        "#########################################################\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def get_ordered_html_files(opf_content):\n",
        "    \"\"\"\n",
        "    Parses the content.opf file to determine the reading order of HTML files in the EPUB.\n",
        "\n",
        "    The function reads the 'content.opf' file, which contains metadata about the EPUB's structure.\n",
        "    It identifies the 'spine' element, which lists the reading order of the content documents,\n",
        "    and the 'manifest' element, which provides the location of these documents.\n",
        "    The function returns a list of HTML file paths in the order they should be read.\n",
        "\n",
        "    Args:\n",
        "    opf_content (str): A string representation of the content.opf file.\n",
        "\n",
        "    Returns:\n",
        "    list: An ordered list of HTML file paths as specified in the EPUB's spine.\n",
        "    \"\"\"\n",
        "\n",
        "    # Parse the content.opf XML content\n",
        "    tree = ET.ElementTree(ET.fromstring(opf_content))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Define the namespace for the OPF package file\n",
        "    ns = {'opf': 'http://www.idpf.org/2007/opf'}\n",
        "\n",
        "    # Find the spine element which indicates the order of the content documents\n",
        "    spine = root.find('opf:spine', ns)\n",
        "    itemrefs = spine.findall('opf:itemref', ns)\n",
        "\n",
        "    # Extract the id references for each item in the spine\n",
        "    item_ids = [itemref.get('idref') for itemref in itemrefs]\n",
        "\n",
        "    # Find the manifest element which lists all the content documents\n",
        "    manifest = root.find('opf:manifest', ns)\n",
        "    items = manifest.findall('opf:item', ns)\n",
        "\n",
        "    # Create a dictionary mapping item IDs to their corresponding file paths\n",
        "    html_files = {item.get('id'): item.get('href') for item in items if item.get('media-type') == 'application/xhtml+xml'}\n",
        "\n",
        "    # Generate an ordered list of HTML files based on the spine order\n",
        "    ordered_html_files = [html_files[item_id] for item_id in item_ids if item_id in html_files]\n",
        "\n",
        "    return ordered_html_files\n",
        "\n",
        "\n",
        "# def extract_text_from_html(html_content):\n",
        "#     \"\"\"\n",
        "#     Extracts and returns text from an HTML content.\n",
        "#     \"\"\"\n",
        "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
        "#     return soup.get_text()\n",
        "\n",
        "def extract_text_from_html(html_content):\n",
        "    \"\"\"\n",
        "    Extracts and returns text from an HTML content.\n",
        "    \"\"\"\n",
        "    #print(\"HTML Content before BeautifulSoup Parsing:\\n\", html_content[:500])  # Print first 500 characters of HTML\n",
        "    print(f\"\\nlen(HTML Content before BeautifulSoup Parsing) -> {len(html_content)}\")  # Print first 500 characters of HTML\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    parsed_text = soup.get_text()\n",
        "    # print(\"Extracted Text:\\n\", parsed_text[:500])  # Print first 500 characters of extracted text\n",
        "    print(f\"\\nLen(Extracted Text) -> {len(parsed_text)}\")  # Print first 500 characters of extracted text\n",
        "\n",
        "    return parsed_text\n",
        "\n",
        "def fix_text_formatting(text):\n",
        "    \"\"\"Replaces the Unicode right single quotation mark with a standard apostrophe.\"\"\"\n",
        "    return text.replace(\"\\u2019\", \"'\")\n",
        "\n",
        "\n",
        "def extract_text_from_epub(epub_file_path, output_jsonl_path, output_json_dir, output_whole_txt_path, output_txt_dir):\n",
        "    with zipfile.ZipFile(epub_file_path, 'r') as epub:\n",
        "        print(\"EPUB Contents:\", epub.namelist())\n",
        "\n",
        "        opf_file = [f for f in epub.namelist() if 'content.opf' in f][0]\n",
        "        opf_content = epub.read(opf_file).decode('utf-8')\n",
        "\n",
        "        ordered_html_files = get_ordered_html_files(opf_content)\n",
        "\n",
        "        # Create a directory for individual JSON files\n",
        "        if not os.path.exists(output_json_dir):\n",
        "            os.makedirs(output_json_dir)\n",
        "\n",
        "        # Create a directory for individual txt files\n",
        "        if not os.path.exists(output_txt_dir):\n",
        "            os.makedirs(output_txt_dir)\n",
        "\n",
        "        # Read and extract text from each HTML file\n",
        "        for html_file in ordered_html_files:\n",
        "            full_path = os.path.join(os.path.dirname(opf_file), html_file)\n",
        "            if full_path in epub.namelist():\n",
        "                html_content = epub.read(full_path).decode('utf-8')\n",
        "\n",
        "                #########################\n",
        "                # extract text from epub\n",
        "                #########################\n",
        "                raw_text = extract_text_from_html(html_content)\n",
        "                print(f\"len(text for json)-> {len(raw_text)}\")\n",
        "\n",
        "                # fix text formatting\n",
        "                text = fix_text_formatting(raw_text)\n",
        "\n",
        "                # Write/Append to a single JSONL file\n",
        "                with open(output_jsonl_path, 'a') as f:\n",
        "                    json_record = json.dumps({'text': text.strip()})\n",
        "                    f.write(json_record + '\\n')\n",
        "\n",
        "                # Save individual JSON file\n",
        "                individual_json_path = os.path.join(output_json_dir, f\"{os.path.splitext(html_file)[0]}.json\")\n",
        "                with open(individual_json_path, 'w') as f:\n",
        "                    json.dump({'text': text.strip()}, f, indent=4)\n",
        "\n",
        "                # Write/Append to a single text .txt file\n",
        "                with open(output_whole_txt_path, 'a') as f:\n",
        "                    f.write(text + '\\n\\n')\n",
        "\n",
        "                # Save individual txt files\n",
        "                individual_txt_path = os.path.join(output_txt_dir, f\"{os.path.splitext(html_file)[0]}.txt\")\n",
        "                with open(individual_txt_path, 'w') as f:\n",
        "                    f.write(text)\n",
        "\n",
        "                print(f\"{html_file} -> ok!\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Warning: File {full_path} not found in the archive.\")\n",
        "\n",
        "\n",
        "def zip_folder(path_to_directory_to_zip='individual_jsons', output_destination_zip_file_path='jsons_archive_zip'):\n",
        "    \"\"\"Creates a zip archive of a specified folder.\n",
        "\n",
        "    Args:\n",
        "        path_to_directory_to_zip (str): The path to the folder to be zipped.\n",
        "        output_destination_zip_file_path (str): The desired name and path of the output zip file.\n",
        "    \"\"\"\n",
        "    # Specify the folder you want to zip\n",
        "    path_to_directory_to_zip = \"individual_jsons\"\n",
        "\n",
        "    # Specify the desired output zip file name (e.g., 'jsons_archive.zip')\n",
        "    output_destination_zip_file_path = \"jsons_archive_zip\"\n",
        "\n",
        "    shutil.make_archive(output_destination_zip_file_path, 'zip', path_to_directory_to_zip)\n",
        "\n",
        "\n",
        "################\n",
        "# Example usage\n",
        "################\n",
        "epub_file_path = 'rustforrustaceans.epub' # Replace with your EPUB file path\n",
        "\n",
        "# json\n",
        "output_jsonl_path = 'output.jsonl'\n",
        "output_json_dir = 'individual_jsons' # Directory to store individual JSON files\n",
        "\n",
        "# txt\n",
        "output_whole_txt_path = 'whole.txt'\n",
        "output_txt_dir = 'individual_txt' # Directory to store individual txt files\n",
        "\n",
        "# run\n",
        "extract_text_from_epub(epub_file_path, output_jsonl_path, output_json_dir, output_whole_txt_path, output_txt_dir)\n",
        "\n",
        "# Call the zip function\n",
        "zip_folder(output_json_dir, 'jsons_archive_zip')\n",
        "zip_folder(output_txt_dir, 'txt_archive_zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywO_m5TMLFlN",
        "outputId": "47850351-0cba-4229-c118-fedde3c7b0c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPUB Contents: ['mimetype', 'META-INF/', 'META-INF/container.xml', 'OPS/', 'OPS/b01.xhtml', 'OPS/c05.xhtml', 'OPS/c07.xhtml', 'OPS/c03.xhtml', 'OPS/c01.xhtml', 'OPS/c06.xhtml', 'OPS/c04.xhtml', 'OPS/toc.xhtml', 'OPS/c02.xhtml', 'OPS/f02.xhtml', 'OPS/image_fi/', 'OPS/image_fi/book_art/', 'OPS/image_fi/book_art/NSAnnotations500-Mono.otf', 'OPS/image_fi/book_art/chapterart.png', 'OPS/image_fi/book_art/nsp_logo_black_rk.png', 'OPS/image_fi/book_art/NSAnnotations-Mono.otf', 'OPS/image_fi/book_art/nsp_logo_black_no-text.png', 'OPS/image_fi/book_art/cover.png', 'OPS/c08.xhtml', 'OPS/content.opf', 'OPS/cover.xhtml', 'OPS/f04.xhtml', 'OPS/c11.xhtml', 'OPS/c13.xhtml', 'OPS/f06.xhtml', 'OPS/f01.xhtml', 'OPS/NSTemplate_v1.css', 'OPS/f03.xhtml', 'OPS/c12.xhtml', 'OPS/toc.ncx', 'OPS/f07.xhtml', 'OPS/f05.xhtml', 'OPS/c09.xhtml', 'OPS/c10.xhtml']\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 563\n",
            "\n",
            "Len(Extracted Text) -> 17\n",
            "len(text for json)-> 17\n",
            "cover.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 41283\n",
            "\n",
            "Len(Extracted Text) -> 11589\n",
            "len(text for json)-> 11589\n",
            "toc.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 787\n",
            "\n",
            "Len(Extracted Text) -> 112\n",
            "len(text for json)-> 112\n",
            "f01.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 2751\n",
            "\n",
            "Len(Extracted Text) -> 1883\n",
            "len(text for json)-> 1883\n",
            "f02.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 1629\n",
            "\n",
            "Len(Extracted Text) -> 989\n",
            "len(text for json)-> 989\n",
            "f03.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 2892\n",
            "\n",
            "Len(Extracted Text) -> 2172\n",
            "len(text for json)-> 2172\n",
            "f04.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 3335\n",
            "\n",
            "Len(Extracted Text) -> 2729\n",
            "len(text for json)-> 2729\n",
            "f05.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 3550\n",
            "\n",
            "Len(Extracted Text) -> 2931\n",
            "len(text for json)-> 2931\n",
            "f06.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 6779\n",
            "\n",
            "Len(Extracted Text) -> 5401\n",
            "len(text for json)-> 5401\n",
            "f07.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 58702\n",
            "\n",
            "Len(Extracted Text) -> 43896\n",
            "len(text for json)-> 43896\n",
            "c01.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 57604\n",
            "\n",
            "Len(Extracted Text) -> 47185\n",
            "len(text for json)-> 47185\n",
            "c02.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 64340\n",
            "\n",
            "Len(Extracted Text) -> 52119\n",
            "len(text for json)-> 52119\n",
            "c03.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 28692\n",
            "\n",
            "Len(Extracted Text) -> 22556\n",
            "len(text for json)-> 22556\n",
            "c04.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 59199\n",
            "\n",
            "Len(Extracted Text) -> 47577\n",
            "len(text for json)-> 47577\n",
            "c05.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 50489\n",
            "\n",
            "Len(Extracted Text) -> 41111\n",
            "len(text for json)-> 41111\n",
            "c06.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 46276\n",
            "\n",
            "Len(Extracted Text) -> 37929\n",
            "len(text for json)-> 37929\n",
            "c07.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 81274\n",
            "\n",
            "Len(Extracted Text) -> 64485\n",
            "len(text for json)-> 64485\n",
            "c08.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 88273\n",
            "\n",
            "Len(Extracted Text) -> 71742\n",
            "len(text for json)-> 71742\n",
            "c09.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 86201\n",
            "\n",
            "Len(Extracted Text) -> 72040\n",
            "len(text for json)-> 72040\n",
            "c10.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 54199\n",
            "\n",
            "Len(Extracted Text) -> 44186\n",
            "len(text for json)-> 44186\n",
            "c11.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 38311\n",
            "\n",
            "Len(Extracted Text) -> 30514\n",
            "len(text for json)-> 30514\n",
            "c12.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 69219\n",
            "\n",
            "Len(Extracted Text) -> 54479\n",
            "len(text for json)-> 54479\n",
            "c13.xhtml -> ok!\n",
            "\n",
            "len(HTML Content before BeautifulSoup Parsing) -> 66152\n",
            "\n",
            "Len(Extracted Text) -> 13877\n",
            "len(text for json)-> 13877\n",
            "b01.xhtml -> ok!\n"
          ]
        }
      ]
    }
  ]
}